input {
  tcp {
    port => 5000
    codec => json
    type => "application_log"
  }
}

# Input pour Kafka
input {
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["pm-events"]
    codec => json
    group_id => "pocmaster_logstash"
    type => "kafka_event"  # Utilisé pour distinguer les données de Kafka
  }
}

filter {
  if [type] == "kafka_event" {
    json {
        source => "msg"
        target => "parsed_message"
      }
      if [parsed_message] {
        mutate {
          add_field => {
            "key" => "%{[parsed_message][key]}"
            "eventName" => "%{[parsed_message][eventName]}"
            "eventLocation" => "%{[parsed_message][eventLocation]}"
            "eventDescription" => "%{[parsed_message][eventDescription]}"
            "eventDate" => "%{[parsed_message][eventDate]}"
          }
        }
        date {
          match => ["eventDate", "ISO8601"]
          target => "eventDate"
        }

        mutate {
          remove_field => ["parsed_message"]
        }
      }
  }
  if [type] == "application_log" {
      # Traitement spécifique pour les logs d'application
      grok {
        match => { "msg" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:loglevel} %{GREEDYDATA:logmessage}" }
      }
      date {
        match => ["timestamp", "ISO8601"]
        target => "@timestamp"
      }
  }
}
output {
  elasticsearch {
    index => "logstash-%{+YYYY.MM.dd}"
    hosts => "${ELASTIC_HOSTS}"
    user => "${ELASTIC_USER}"
    password => "${ELASTIC_PASSWORD}"
    cacert => "certs/ca/ca.crt"
  }
  # Ajoute l'affichage de debug dans la console
  stdout { codec => rubydebug }
}